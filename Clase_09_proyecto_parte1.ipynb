{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/lacamposm/course-fundamentals-llms-openai-langchain/raw/main/images/image_igac.jpg\" alt=\"Imagen_IGAC\" width=\"280\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ***Fundamentos de LLMs con Python: Explorando ChatGPT y LangChain***\n",
    "\n",
    "---\n",
    "\n",
    "#### ***Instructor: [Luis Andr茅s Campos Maldonado](https://www.linkedin.com/in/lacamposm/)***\n",
    "\n",
    "##### ***Email: luisandres.campos@igac.gov.co***\n",
    "\n",
    "##### ***Contratista-Observatorio Inmobiliario Catastral***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ***Clase 09 - 02 de mayo de 2024***\n",
    "---\n",
    "\n",
    "## ***Taller Pr谩ctico de Implementaci贸n con Streamlit - Parte 1***\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "\n",
    "- Revisar brevemente los conceptos clave aprendidos durante el curso y c贸mo se integrar谩n en el proyecto final.\n",
    "- Comenzar el desarrollo de una aplicaci贸n con Streamlit que integre las   funcionalidades   de   LangChain,   enfoc谩ndose   en   la   estructura b谩sica   y   la   incorporaci贸n   de   memoria   y   agentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Mini proyecto de final de curso***\n",
    "\n",
    "\n",
    "### ***Descripci贸n de la Tarea.***\n",
    "Desarrollar una aplicaci贸n de chatbot con Streamlit que integre capacidades avanzadas como b煤squeda en internet y RAG (Retrieval-Augmented Generation). La aplicaci贸n permitir谩 subir documentos PDF y utilizar la informaci贸n contenida en estos para generar respuestas contextualizadas, soportadas por un sistema de memoria que permite al chatbot recordar interacciones pasadas.\n",
    "\n",
    "Esta tarea combina varias tecnolog铆as avanzadas de procesamiento de lenguaje natural, proporcionando experiencia pr谩ctica en la creaci贸n de aplicaciones interactivas e inteligentes. Al integrar RAG y capacidades de b煤squeda, los estudiantes aprenden a manejar y sintetizar grandes vol煤menes de informaci贸n de manera eficaz, mientras que la implementaci贸n de memoria en chatbots representa un avance significativo en la creaci贸n de sistemas m谩s naturales y 煤tiles para los usuarios finales. Adem谩s, desarrollar esta aplicaci贸n en Streamlit facilita la visualizaci贸n y la interacci贸n directa, habilidades esenciales para cualquier desarro en el campo de la AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Primera parte.***\n",
    "\n",
    "Construir una APP que hago de RAG. Use el notebook _APP-RAG-OIC_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/app_final_proyect.ipynb) para la construcci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 1.***\n",
    "\n",
    "Cree la primera parte de la interfaz de usuario con el siguiente c贸digo:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import shutil\n",
    "import streamlit as st\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "dir_pdf_documents = \"RAG\"\n",
    "\n",
    "# Interfaz de usuario #\n",
    "st.title(\" Chatbot with RAG - OIC \")\n",
    "\n",
    "if \"chat_active\" not in st.session_state:\n",
    "    with st.sidebar.expander(f\"Seleccione modelo y cargue sus archivos.\"):\n",
    "        model_openai = st.radio(\n",
    "            \"Seleccione su ChatModel:\",\n",
    "            [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\"],\n",
    "            key=\"openai_chat_model\"\n",
    "        )\n",
    "        st.session_state[\"openai_model\"] = model_openai\n",
    "        if os.path.exists(dir_pdf_documents):\n",
    "            delete_dir = st.button(f\"Borrar carpeta {dir_pdf_documents}\")\n",
    "            if delete_dir:\n",
    "                shutil.rmtree(dir_pdf_documents)\n",
    "                st.rerun()\n",
    "\n",
    "        uploaded_files = st.file_uploader(\"Elige los archivos PDF para el RAG\", accept_multiple_files=True, type=[\"pdf\"])\n",
    "\n",
    "        if st.button(\"Finalizar carga de archivos y Chatear\"):\n",
    "            st.session_state[\"chat_active\"] = True\n",
    "            if len(uploaded_files) > 0:\n",
    "                os.makedirs(dir_pdf_documents)\n",
    "                for uploaded_file in uploaded_files:\n",
    "                    bytes_data = uploaded_file.read()\n",
    "                    path_file = os.path.join(dir_pdf_documents, uploaded_file.name)                \n",
    "                    with open(path_file, \"wb\") as f:\n",
    "                        f.write(bytes_data)\n",
    "                        st.write(f\"Archivo {uploaded_file.name} guardado\")\n",
    "                # retriever = create_retriver_documents()\n",
    "                # st.session_state[\"retriever\"] = retriever\n",
    "                # st.rerun()\n",
    "                \n",
    "            else:\n",
    "                st.rerun()\n",
    "# =================================================================================== #\n",
    "# ====   Creamos secci贸n de chatbot con Memory en session_state de streamlit.  ====== #\n",
    "# =================================================================================== #\n",
    "```\n",
    "\n",
    "Apoye de la lectura: _app-oic - Memory_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/app.ipynb)\n",
    "\n",
    "Observe las funcionalidades que le ofrece. Comente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 2:***\n",
    "\n",
    "Recupere las funciones: \n",
    "\n",
    "1. ```chatbot_template```\n",
    "2. ```stream_response_with_memory_openai```\n",
    "\n",
    "de: _app-oic - Memory_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/app.ipynb)\n",
    "\n",
    "Adicionelas justo despu茅s de importaci贸n de librerias y antes de iniciar la interfaz de usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 3:***\n",
    "\n",
    "Construya la funci贸n ```create_retriver_documents()```:\n",
    "\n",
    "```python\n",
    "def create_retriver_documents():\n",
    "    \"\"\"\n",
    "    Crea un recuperador de documentos a partir de archivos PDF almacenados en un directorio especificado.\n",
    "\n",
    "    Esta funci贸n carga documentos desde un directorio utilizando PyPDFDirectoryLoader, divide el\n",
    "    texto en segmentos manejables mediante RecursiveCharacterTextSplitter y luego incrusta\n",
    "    estos segmentos usando el modelo de incrustaci贸n de texto de OpenAI. Los segmentos de texto incrustados se\n",
    "    almacenan en un almacenamiento vectorial (Chroma), que luego se utiliza para crear un recuperador para\n",
    "    la recuperaci贸n eficiente de documentos basada en la similitud vectorial.\n",
    "\n",
    "    Return:\n",
    "        Retriever: Un objeto capaz de recuperar documentos basado en la similitud de las consultas.\n",
    "    \"\"\"\n",
    "```\n",
    "Haga uso de las siguientes lineales de c贸digo al iniciar la funci贸n:\n",
    "\n",
    "```python\n",
    "loader = PyPDFDirectoryLoader(dir_pdf_documents)\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "Para el resto de la funci贸n tome como gu铆a lo desarrollando en - _Lectura clase 6: RAG_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/Clase_06_retrieval_and_rag.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 4.***\n",
    "\n",
    "Construya la funci贸n ```stream_rag_with_memory_openai(model_name, query, chat_history, retriever)```\n",
    "\n",
    "```python\n",
    "def stream_rag_with_memory_openai(model_name, query, chat_history, retriever):\n",
    "    \"\"\"\n",
    "    Ejecuta un proceso de generaci贸n de respuestas aumentado por la recuperaci贸n de informaci贸n (RAG) \n",
    "    con memoria, utilizando el modelo especificado de OpenAI.\n",
    "\n",
    "    Par谩metros:\n",
    "        model_name (str): El nombre del modelo de OpenAI a utilizar.\n",
    "        query (str): La consulta del usuario a responder.\n",
    "        chat_history (list): Historial de chat que puede incluir mensajes previos para mantener el contexto.\n",
    "        retriever: Un objeto recuperador que accede a la informaci贸n relevante para responder la consulta.\n",
    "\n",
    "    Descripci贸n:\n",
    "    La funci贸n configura una cadena de procesamiento donde el contexto y la historia del chat se utilizan para\n",
    "    formular una respuesta relevante y contextualizada. Se utiliza un prompt espec铆fico que considera\n",
    "    el contexto de catastro en Colombia para estructurar la respuesta.\n",
    "\n",
    "    Retorna:\n",
    "        stream: Una cadena de ejecuci贸n que procesa la consulta en tiempo real y devuelve una respuesta.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Tome como referencia la funci贸n ```stream_response_with_memory_openai``` y la _Lectura clase 6: RAG_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/Clase_06_retrieval_and_rag.ipynb), para recuperar el ```rag_chain```. Adicione en el diccionario del pipeline lo siguiente: ```\"chat_history\": lambda x: chat_history```\n",
    "\n",
    "Coloque esta funci贸n justo despu茅s de la tambi茅n funci贸n: ```stream_response_with_memory_openai```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 5.***\n",
    "\n",
    "Descomente las lineas de codigo en el paso y complete la APP agregando y  :\n",
    "\n",
    "```python\n",
    "if \"chat_active\" in st.session_state and os.path.exists(save_rag_directory):\n",
    "    st.text(f\"Archivos cargados para chatear:\\n {', '.join(os.listdir(save_rag_directory))}\")\n",
    "\n",
    "# Iniciamos un chat history #\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "for message in st.session_state.chat_history:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"Human\"):\n",
    "            st.markdown(message.content)\n",
    "    else:\n",
    "        with st.chat_message(\"AI\"):\n",
    "            st.markdown(message.content)\n",
    "\n",
    "\n",
    "user_query = st.chat_input(\"Your message\")\n",
    "\n",
    "if user_query is not None and user_query != \"\":\n",
    "    st.session_state.chat_history.append(HumanMessage(user_query))\n",
    "\n",
    "    with st.chat_message(\"Human\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"AI\"):\n",
    "        if os.path.exists(save_rag_directory):        \n",
    "            llm_response = st.write_stream(\n",
    "                stream_rag_with_memory_openai(\n",
    "                    model_name=st.session_state[\"openai_model\"],\n",
    "                    query=user_query,\n",
    "                    chat_history=st.session_state.chat_history,\n",
    "                    retriever=st.session_state[\"retriever\"]\n",
    "                    )\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            llm_response = st.write_stream(\n",
    "            stream_response_with_memory_openai(\n",
    "                model_name=st.session_state[\"openai_model\"],\n",
    "                query=user_query,\n",
    "                chat_history=st.session_state.chat_history)\n",
    "            )\n",
    "    \n",
    "    st.session_state.chat_history.append(AIMessage(llm_response))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 06***\n",
    "\n",
    "Haga uso de la APP:\n",
    "\n",
    " Chatbot with RAG - OIC "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
