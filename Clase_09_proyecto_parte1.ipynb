{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/lacamposm/course-fundamentals-llms-openai-langchain/raw/main/images/image_igac.jpg\" alt=\"Imagen_IGAC\" width=\"280\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ***Fundamentos de LLMs con Python: Explorando ChatGPT y LangChain***\n",
    "\n",
    "---\n",
    "\n",
    "#### ***Instructor: [Luis Andrés Campos Maldonado](https://www.linkedin.com/in/lacamposm/)***\n",
    "\n",
    "##### ***Email: luisandres.campos@igac.gov.co***\n",
    "\n",
    "##### ***Contratista-Observatorio Inmobiliario Catastral***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ***Clase 09 - 02 de mayo de 2024***\n",
    "---\n",
    "\n",
    "## ***Taller Práctico de Implementación con Streamlit - Parte 1***\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "\n",
    "- Revisar brevemente los conceptos clave aprendidos durante el curso y cómo se integrarán en el proyecto final.\n",
    "- Comenzar el desarrollo de una aplicación con Streamlit que integre las   funcionalidades   de   LangChain,   enfocándose   en   la   estructura básica   y   la   incorporación   de   memoria   y   agentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Mini proyecto de final de curso***\n",
    "\n",
    "\n",
    "### ***Descripción de la Tarea.***\n",
    "Desarrollar una aplicación de chatbot con Streamlit que integre capacidades avanzadas como búsqueda en internet y RAG (Retrieval-Augmented Generation). La aplicación permitirá subir documentos PDF y utilizar la información contenida en estos para generar respuestas contextualizadas, soportadas por un sistema de memoria que permite al chatbot recordar interacciones pasadas.\n",
    "\n",
    "Esta tarea combina varias tecnologías avanzadas de procesamiento de lenguaje natural, proporcionando experiencia práctica en la creación de aplicaciones interactivas e inteligentes. Al integrar RAG y capacidades de búsqueda, los estudiantes aprenden a manejar y sintetizar grandes volúmenes de información de manera eficaz, mientras que la implementación de memoria en chatbots representa un avance significativo en la creación de sistemas más naturales y útiles para los usuarios finales. Además, desarrollar esta aplicación en Streamlit facilita la visualización y la interacción directa, habilidades esenciales para cualquier desarro en el campo de la AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Primera parte.***\n",
    "\n",
    "Construir una APP que hago de RAG. Use el notebook _APP-RAG-OIC_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/app_final_proyect.ipynb) para la construcción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 1.***\n",
    "\n",
    "Cree la primera parte de la interfaz de usuario con el siguiente código:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import shutil\n",
    "import streamlit as st\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "dir_pdf_documents = \"RAG\"\n",
    "\n",
    "# Interfaz de usuario #\n",
    "st.title(\"🦜 Chatbot with RAG - OIC 🦜\")\n",
    "\n",
    "if \"chat_active\" not in st.session_state:\n",
    "    with st.sidebar.expander(f\"Seleccione modelo y cargue sus archivos.\"):\n",
    "        model_openai = st.radio(\n",
    "            \"Seleccione su ChatModel:\",\n",
    "            [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\"],\n",
    "            key=\"openai_chat_model\"\n",
    "        )\n",
    "        st.session_state[\"openai_model\"] = model_openai\n",
    "        if os.path.exists(dir_pdf_documents):\n",
    "            delete_dir = st.button(f\"Borrar carpeta {dir_pdf_documents}\")\n",
    "            if delete_dir:\n",
    "                shutil.rmtree(dir_pdf_documents)\n",
    "                st.rerun()\n",
    "\n",
    "        uploaded_files = st.file_uploader(\"Elige los archivos PDF para el RAG\", accept_multiple_files=True, type=[\"pdf\"])\n",
    "\n",
    "        if st.button(\"Finalizar carga de archivos y Chatear\"):\n",
    "            st.session_state[\"chat_active\"] = True\n",
    "            if len(uploaded_files) > 0:\n",
    "                os.makedirs(dir_pdf_documents)\n",
    "                for uploaded_file in uploaded_files:\n",
    "                    bytes_data = uploaded_file.read()\n",
    "                    path_file = os.path.join(dir_pdf_documents, uploaded_file.name)                \n",
    "                    with open(path_file, \"wb\") as f:\n",
    "                        f.write(bytes_data)\n",
    "                        st.write(f\"Archivo {uploaded_file.name} guardado\")\n",
    "                # retriever = create_retriver_documents()\n",
    "                # st.session_state[\"retriever\"] = retriever\n",
    "                # st.rerun()\n",
    "                \n",
    "            else:\n",
    "                st.rerun()\n",
    "# =================================================================================== #\n",
    "# ====   Creamos sección de chatbot con Memory en session_state de streamlit.  ====== #\n",
    "# =================================================================================== #\n",
    "```\n",
    "\n",
    "Apoye de la lectura: _app-oic - Memory_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/app.ipynb)\n",
    "\n",
    "Observe las funcionalidades que le ofrece. Comente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 2:***\n",
    "\n",
    "Recupere las funciones: \n",
    "\n",
    "1. ```chatbot_template```\n",
    "2. ```stream_response_with_memory_openai```\n",
    "\n",
    "de: _app-oic - Memory_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/app.ipynb)\n",
    "\n",
    "Adicionelas justo después de importación de librerias y antes de iniciar la interfaz de usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 3:***\n",
    "\n",
    "Construya la función ```create_retriver_documents()```:\n",
    "\n",
    "```python\n",
    "def create_retriver_documents():\n",
    "    \"\"\"\n",
    "    Crea un recuperador de documentos a partir de archivos PDF almacenados en un directorio especificado.\n",
    "\n",
    "    Esta función carga documentos desde un directorio utilizando PyPDFDirectoryLoader, divide el\n",
    "    texto en segmentos manejables mediante RecursiveCharacterTextSplitter y luego incrusta\n",
    "    estos segmentos usando el modelo de incrustación de texto de OpenAI. Los segmentos de texto incrustados se\n",
    "    almacenan en un almacenamiento vectorial (Chroma), que luego se utiliza para crear un recuperador para\n",
    "    la recuperación eficiente de documentos basada en la similitud vectorial.\n",
    "\n",
    "    Return:\n",
    "        Retriever: Un objeto capaz de recuperar documentos basado en la similitud de las consultas.\n",
    "    \"\"\"\n",
    "```\n",
    "Haga uso de las siguientes lineales de código al iniciar la función:\n",
    "\n",
    "```python\n",
    "loader = PyPDFDirectoryLoader(dir_pdf_documents)\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "Para el resto de la función tome como guía lo desarrollando en - _Lectura clase 6: RAG_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/Clase_06_retrieval_and_rag.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 4.***\n",
    "\n",
    "Construya la función ```stream_rag_with_memory_openai(model_name, query, chat_history, retriever)```\n",
    "\n",
    "```python\n",
    "def stream_rag_with_memory_openai(model_name, query, chat_history, retriever):\n",
    "    \"\"\"\n",
    "    Ejecuta un proceso de generación de respuestas aumentado por la recuperación de información (RAG) \n",
    "    con memoria, utilizando el modelo especificado de OpenAI.\n",
    "\n",
    "    Parámetros:\n",
    "        model_name (str): El nombre del modelo de OpenAI a utilizar.\n",
    "        query (str): La consulta del usuario a responder.\n",
    "        chat_history (list): Historial de chat que puede incluir mensajes previos para mantener el contexto.\n",
    "        retriever: Un objeto recuperador que accede a la información relevante para responder la consulta.\n",
    "\n",
    "    Descripción:\n",
    "    La función configura una cadena de procesamiento donde el contexto y la historia del chat se utilizan para\n",
    "    formular una respuesta relevante y contextualizada. Se utiliza un prompt específico que considera\n",
    "    el contexto de catastro en Colombia para estructurar la respuesta.\n",
    "\n",
    "    Retorna:\n",
    "        stream: Una cadena de ejecución que procesa la consulta en tiempo real y devuelve una respuesta.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Tome como referencia la función ```stream_response_with_memory_openai``` y la _Lectura clase 6: RAG_ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lacamposm/course-fundamentals-llms-openai-langchain/blob/main/Clase_06_retrieval_and_rag.ipynb), para recuperar el ```rag_chain```. Adicione en el diccionario del pipeline lo siguiente: ```\"chat_history\": lambda x: chat_history```\n",
    "\n",
    "Coloque esta función justo después de la también función: ```stream_response_with_memory_openai```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 5.***\n",
    "\n",
    "Descomente las lineas de codigo en el paso y complete la APP agregando y  :\n",
    "\n",
    "```python\n",
    "if \"chat_active\" in st.session_state and os.path.exists(save_rag_directory):\n",
    "    st.text(f\"Archivos cargados para chatear:\\n {', '.join(os.listdir(save_rag_directory))}\")\n",
    "\n",
    "# Iniciamos un chat history #\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "for message in st.session_state.chat_history:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"Human\"):\n",
    "            st.markdown(message.content)\n",
    "    else:\n",
    "        with st.chat_message(\"AI\"):\n",
    "            st.markdown(message.content)\n",
    "\n",
    "\n",
    "user_query = st.chat_input(\"Your message\")\n",
    "\n",
    "if user_query is not None and user_query != \"\":\n",
    "    st.session_state.chat_history.append(HumanMessage(user_query))\n",
    "\n",
    "    with st.chat_message(\"Human\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"AI\"):\n",
    "        if os.path.exists(save_rag_directory):        \n",
    "            llm_response = st.write_stream(\n",
    "                stream_rag_with_memory_openai(\n",
    "                    model_name=st.session_state[\"openai_model\"],\n",
    "                    query=user_query,\n",
    "                    chat_history=st.session_state.chat_history,\n",
    "                    retriever=st.session_state[\"retriever\"]\n",
    "                    )\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            llm_response = st.write_stream(\n",
    "            stream_response_with_memory_openai(\n",
    "                model_name=st.session_state[\"openai_model\"],\n",
    "                query=user_query,\n",
    "                chat_history=st.session_state.chat_history)\n",
    "            )\n",
    "    \n",
    "    st.session_state.chat_history.append(AIMessage(llm_response))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paso 06***\n",
    "\n",
    "Haga uso de la APP:\n",
    "\n",
    "🦜 Chatbot with RAG - OIC 🦜"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
