{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmc_s2ezvU0"
      },
      "source": [
        "# ***Chatbot - OIC***\n",
        "\n",
        "\n",
        "Desarrolado por [Andres Campos](https://www.linkedin.com/in/lacamposm/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvlYkCQ9vFiy",
        "outputId": "35812c7f-f4b9-4e07-b8d5-356f49674319"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pandas openpyxl langchain openai langchain-openai langchain-community langchain-core langchain-text-splitters chromadb pypdf\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waCfwniZOow8"
      },
      "source": [
        "## ***Creamos nuestra app de streamlit.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meJ36PefNftd",
        "outputId": "9a1c9874-49a5-44fc-eadd-bfa8f79f02ca"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import logging\n",
        "import streamlit as st\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "\n",
        "\n",
        "def chatbot_template(system_message, user_question):\n",
        "    \"\"\"\n",
        "    Create a template to pass custom queries to LLM\n",
        "    :param user_question: Message user.\n",
        "    :param system_message: Message to the system, like a rol.\n",
        "    :return: Configured with the system message, chat history, and user question.\n",
        "    \"\"\"\n",
        "    chat_prompt = ChatPromptTemplate(\n",
        "        messages=[\n",
        "            SystemMessagePromptTemplate.from_template(system_message),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            HumanMessagePromptTemplate.from_template(user_question)\n",
        "        ]\n",
        "    )\n",
        "    return chat_prompt\n",
        "\n",
        "\n",
        "def stream_response_with_memory_openai(model_name, query, chat_history):\n",
        "    \"\"\"\n",
        "    Streams a response for a given query using an OpenAI model, formatted to maintain conversation history.\n",
        "\n",
        "    :param model_name: The name of the OpenAI model to use for generating responses.\n",
        "    :param query: The current user question or input.\n",
        "    :param chat_history: The history of previous interactions to be considered for context.\n",
        "    :return: A streaming object that continuously provides the chatbot's responses.\n",
        "    \"\"\"\n",
        "    system_prompt = \"\"\"You are a friendly chatbot having a conversation with a human and giving answers only in \\\n",
        "    Spanish.\"\"\"\n",
        "    user_question = \"{user_question}\"\n",
        "    prompt = chatbot_template(system_message=system_prompt, user_question=user_question)\n",
        "\n",
        "    chat_openai = ChatOpenAI(\n",
        "        model=model_name,\n",
        "        temperature=0.1,\n",
        "    )\n",
        "\n",
        "    chain = prompt | chat_openai | StrOutputParser()\n",
        "\n",
        "    return chain.stream(input={\"chat_history\": chat_history, \"user_question\": query})\n",
        "\n",
        "\n",
        "# Interfaz de usuario con streamlit #\n",
        "st.title(\"ü¶ú Chatbot - OIC ü¶ú\")\n",
        "with st.sidebar.expander(\"Modelos de OPENAI\"):\n",
        "    model_openai = st.radio(\n",
        "        \"Seleccione su ChatModel:\",\n",
        "        [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\"],\n",
        "        key=\"openai_chat_model\"\n",
        "    )\n",
        "    st.session_state[\"openai_model\"] = model_openai\n",
        "\n",
        "# Iniciamos un chat history #\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "\n",
        "for message in st.session_state.chat_history:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.markdown(message.content)\n",
        "    else:\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.markdown(message.content)\n",
        "\n",
        "user_query = st.chat_input(\"Your message\")\n",
        "\n",
        "if user_query is not None and user_query != \"\":\n",
        "    logging.info(\"Inicio del chat\")\n",
        "    st.session_state.chat_history.append(HumanMessage(user_query))\n",
        "\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    with st.chat_message(\"AI\"):\n",
        "        llm_response = st.write_stream(\n",
        "            stream_response_with_memory_openai(\n",
        "                model_name=st.session_state[\"openai_model\"],\n",
        "                query=user_query,\n",
        "                chat_history=st.session_state.chat_history)\n",
        "        )\n",
        "    st.session_state.chat_history.append(AIMessage(llm_response))\n",
        "    logging.info(\"Fin del chat.\\n==============================================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZlEJkmSOoxC"
      },
      "source": [
        "## ***Install localtunnel***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAyqQCQVOoxC",
        "outputId": "0da85142-1ac5-4734-f779-680fc15c3d8f"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kccYE2lkN20y"
      },
      "source": [
        "## ***Run streamlit in background***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv912rRAN0fs"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_KW9juhOCuH"
      },
      "source": [
        "## ***Exponer el puerto 8501.***\n",
        "\n",
        "Se crear√° un archivo `log.txt` donde obtendr√° la direcci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTGAizLhOIgC",
        "outputId": "826a0ec8-8126-45e1-8344-617b0eee1467"
      },
      "outputs": [],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Lograr que funcione en colab fue tomado de: [discuciones de streamlit](https://discuss.streamlit.io/t/how-to-launch-streamlit-app-from-google-colab-notebook/42399/3)***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
