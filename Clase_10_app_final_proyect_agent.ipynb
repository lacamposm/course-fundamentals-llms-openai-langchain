{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/lacamposm/course-fundamentals-llms-openai-langchain/raw/main/images/image_igac.jpg\" alt=\"Imagen_IGAC\" width=\"280\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ***Fundamentos de LLMs con Python: Explorando ChatGPT y LangChain***\n",
    "\n",
    "---\n",
    "\n",
    "#### ***Instructor: [Luis Andrés Campos Maldonado](https://www.linkedin.com/in/lacamposm/)***\n",
    "\n",
    "##### ***Email: luisandres.campos@igac.gov.co***\n",
    "\n",
    "##### ***Contratista-Observatorio Inmobiliario Catastral***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ***Clase 10 - 10 de mayo de 2024***\n",
    "---\n",
    "\n",
    "## ***Taller Práctico de Implementación con Streamlit - Parte 2***\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "\n",
    "- Continuar   y   finalizar   el   desarrollo   de   la   aplicación   Streamlit, integrando todas las funcionalidades discutidas durante el curso.\n",
    "\n",
    "- Realizar   pruebas,   depuración   y   optimización   de   la   aplicación desarrollada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas openpyxl langchain openai langchain-openai langchain-community langchain-core langchain-text-splitters \n",
    "!pip install streamlit chromadb pypdf\n",
    "!pip install -U duckduckgo_search==5.3.0b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app_final_proyect_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app_final_proyect_agent.py\n",
    "import os\n",
    "import shutil\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "## Tools and agents\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "save_rag_directory = \"RAG\"\n",
    "\n",
    "\n",
    "def save_uploaded_files(uploaded_files):\n",
    "    \"\"\"\n",
    "    Function that saves uploaded files in a specific directory and creates a document retriever.\n",
    "    \n",
    "    :param uploaded_files: list of files uploaded by the user.\n",
    "    :param save_rag_directory: directory where the files will be saved.\n",
    "    \n",
    "    Returns: None    \n",
    "    \"\"\"\n",
    "    if len(uploaded_files) > 0:\n",
    "        os.makedirs(save_rag_directory, exist_ok=True)\n",
    "        for uploaded_file in uploaded_files:\n",
    "            bytes_data = uploaded_file.read()\n",
    "            path_file = os.path.join(save_rag_directory, uploaded_file.name)\n",
    "            with open(path_file, \"wb\") as f:\n",
    "                f.write(bytes_data)\n",
    "                st.write(f\"Archivo {uploaded_file.name} guardado\")\n",
    "\n",
    "        st.session_state[\"retriever\"] = create_retriever_documents()\n",
    "        st.session_state[\"agent_executor\"] = create_agent_chat_with_memory()\n",
    "        st.rerun()\n",
    "    else:\n",
    "        st.rerun()\n",
    "\n",
    "def chatbot_template(system_message, user_question):\n",
    "    \"\"\"\n",
    "    Create a template to pass custom queries to LLM \n",
    "    :param user_question: Message user.\n",
    "    :param system_message: Message to the system, like a rol.\n",
    "    :return: Configured with the system message, chat history, and user question.\n",
    "    \"\"\"\n",
    "    chat_prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            HumanMessagePromptTemplate.from_template(user_question)\n",
    "        ]\n",
    "    )\n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "def create_retriever_documents():\n",
    "    \"\"\"\n",
    "    Creates a document retriever from PDF files stored in a specified directory.\n",
    "\n",
    "    This function loads documents from a directory using PyPDFDirectoryLoader, splits the\n",
    "    text into manageable segments using RecursiveCharacterTextSplitter, and then embeds\n",
    "    these segments using OpenAI's text embedding model. The embedded text segments are\n",
    "    stored in a vector storage (Chroma), which is then used to create a retriever for\n",
    "    efficient document retrieval based on vector similarity.\n",
    "\n",
    "    Return:\n",
    "        Retriever: An object capable of retrieving documents based on query similarity.\n",
    "    \"\"\"\n",
    "    loader = PyPDFDirectoryLoader(save_rag_directory)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=openai_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def stream_response_with_memory_openai(model_name, query, chat_history):\n",
    "    \"\"\"\n",
    "    Streams a response for a given query using an OpenAI model, formatted to maintain conversation history.\n",
    "\n",
    "    :param model_name: The name of the OpenAI model to use for generating responses.\n",
    "    :param query: The current user question or input.\n",
    "    :param chat_history: The history of previous interactions to be considered for context.\n",
    "    :return: A streaming object that continuously provides the chatbot's responses.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a friendly chatbot having a conversation with a human and giving answers only in \\\n",
    "    Spanish.\"\"\"\n",
    "    user_question = \"{user_question}\"\n",
    "    prompt = chatbot_template(system_message=system_prompt, user_question=user_question)\n",
    "    chat_openai = ChatOpenAI(model=model_name)\n",
    "\n",
    "    chain = (\n",
    "            {\"chat_history\": lambda x: chat_history, \"user_question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | chat_openai\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain.stream(query)\n",
    "\n",
    "def create_agent_chat_with_memory():\n",
    "    \"\"\"\n",
    "    Creates and configures a chat agent with memory capabilities integrating specific tools for search and\n",
    "    information retrieval, using a language model for generating responses.\n",
    "\n",
    "    This function sets up a DuckDuckGo-based search tool and a retrieval tool configured for inquiries related\n",
    "    to multipurpose cadastre in Colombia. It also initializes a conversation memory that stores and retrieves\n",
    "    past conversation messages to maintain context across user interactions.\n",
    "\n",
    "    :param None: This function does not take any parameters.\n",
    "\n",
    "    :return:\n",
    "        AgentExecutor: An executor that manages the lifecycle of the agent, including tool execution\n",
    "        and conversation memory management.\n",
    "\n",
    "    Example:\n",
    "        agent_executor = create_agent_chat_with_memory()\n",
    "        response = agent_executor.invoke({\"user_message\": \"What is a multipurpose cadastre?\"})\n",
    "        print(response[\"output\"])  # Displays the agent's response to the user's question.\n",
    "    \"\"\"\n",
    "    # Search tool.\n",
    "    search_wrapper = DuckDuckGoSearchAPIWrapper()\n",
    "    search_tool = DuckDuckGoSearchRun(api_wrapper=search_wrapper)\n",
    "\n",
    "    # Retrival tool.\n",
    "    retriever_tool = create_retriever_tool(\n",
    "        retriever=st.session_state[\"retriever\"],\n",
    "        name=\"catastro_multiproposito\",\n",
    "        description=\"\"\"Search for information on multipurpose cadastre in Colombia. \n",
    "        If you have any questions about multipurpose cadastre in Colombia, you should use this tool!\"\"\"\n",
    "    )\n",
    "    \n",
    "    tools = [search_tool, retriever_tool]\n",
    "    memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Eres un buen chatbot teniendo una conversación con un humano.\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{user_message}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    agent = create_tool_calling_agent(llm,tools,prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "# =================================== Interfaz de usuario ============================================ #\n",
    "st.title(\"🦜 Agent - WebSearch and RAG 🦜\")\n",
    "\n",
    "if \"chat_active\" not in st.session_state:\n",
    "    with st.sidebar.expander(f\"Seleccione modelo y cargue sus archivos.\"):\n",
    "        model_openai = st.radio(\n",
    "            \"Seleccione su ChatModel:\",\n",
    "            [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\"],\n",
    "            key=\"openai_chat_model\"\n",
    "        )\n",
    "        st.session_state[\"openai_model\"] = model_openai\n",
    "        if os.path.exists(save_rag_directory):\n",
    "            delete_dir = st.button(f\"Borrar carpeta {save_rag_directory}\")\n",
    "            if delete_dir:\n",
    "                shutil.rmtree(save_rag_directory)\n",
    "                st.rerun()\n",
    "\n",
    "        uploaded_files = st.file_uploader(\"Elige los archivos PDF para el RAG\", accept_multiple_files=True,\n",
    "                                          type=[\"pdf\"])\n",
    "\n",
    "        if st.button(\"Finalizar carga de archivos y Chatear\"):\n",
    "            st.session_state[\"chat_active\"] = True\n",
    "            save_uploaded_files(uploaded_files)\n",
    "\n",
    "\n",
    "if \"chat_active\" in st.session_state and \"agent_executor\" in st.session_state:\n",
    "    st.text(f\"Archivos cargados para chatear:\\n {', '.join(os.listdir(save_rag_directory))}\")\n",
    "\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "for message in st.session_state.chat_history:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"Human\"):\n",
    "            st.markdown(message.content)\n",
    "    else:\n",
    "        with st.chat_message(\"AI\"):\n",
    "            st.markdown(message.content)\n",
    "\n",
    "user_query = st.chat_input(\"Your message\")\n",
    "\n",
    "if user_query is not None and user_query != \"\":\n",
    "    st.session_state.chat_history.append(HumanMessage(user_query))\n",
    "\n",
    "    with st.chat_message(\"Human\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"AI\"):\n",
    "            if os.path.exists(save_rag_directory) and \"retriever\" in st.session_state:\n",
    "                agent_executor = st.session_state[\"agent_executor\"]\n",
    "                agent_response = agent_executor.invoke({\"user_message\": user_query})[\"output\"]\n",
    "                st.write(agent_response)\n",
    "            \n",
    "            else:\n",
    "                st.warning(\"No se está usando la capacidad del agente!\")\n",
    "                agent_response = st.write_stream(\n",
    "                    stream_response_with_memory_openai(\n",
    "                        model_name=st.session_state[\"openai_model\"],\n",
    "                        query=user_query,\n",
    "                        chat_history=st.session_state.chat_history)\n",
    "                )\n",
    "    st.session_state.chat_history.append(AIMessage(agent_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run /content/app_final_proyect_agent.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
