{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHqfwjHTrPVe"
   },
   "source": [
    "---\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/lacamposm/course-fundamentals-llms-openai-langchain/raw/main/images/image_igac.jpg\" alt=\"Imagen_IGAC\" width=\"280\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcolQEVcoqh7"
   },
   "source": [
    "---\n",
    "\n",
    "# ***Fundamentos de LLMs con Python: Explorando ChatGPT y LangChain***\n",
    "\n",
    "---\n",
    "\n",
    "#### ***Instructor: [Luis Andrés Campos Maldonado](https://www.linkedin.com/in/lacamposm/)***\n",
    "\n",
    "##### ***Email: luisandres.campos@igac.gov.co***\n",
    "\n",
    "##### ***Contratista-Observatorio Inmobiliario Catastral***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdTcl3zMr7OA"
   },
   "source": [
    "---\n",
    "\n",
    "# ***Clase 08 - 19 de abril de 2024***\n",
    "---\n",
    "\n",
    "## ***El Concepto de Agent en LangChain***\n",
    "\n",
    "**Objetivos de Aprendizaje:**\n",
    "\n",
    "- Introducir el concepto de agentes en LangChain y su relevancia para el desarrollo de aplicaciones de IA.\n",
    "- Explicar cómo diseñar, implementar y gestionar agentes inteligentes que interactúan con modelos de lenguaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Introducción a los Agentes***\n",
    "\n",
    "Un **agente** en LangChain es una entidad sofisticada que, mediante modelos de lenguaje, determina y ejecuta secuencias de acciones basándose en las entradas de los usuarios. Estos agentes contrastan con los procesos tradicionales que dependen de flujos de trabajo estáticos y predefinidos; en lugar de eso, son sistemas dinámicos y adaptativos que utilizan el razonamiento del modelo de lenguaje para evaluar y reaccionar a situaciones en tiempo real. Esta capacidad los hace extremadamente versátiles, permitiéndoles ajustar sus comportamientos y respuestas de acuerdo con el contexto específico y los datos entrantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Características de un agente en LangChain:***\n",
    "\n",
    "- **Autonomía:** Los agentes operan sin intervención humana directa una vez que están configurados.\n",
    "- **Adaptabilidad:** Cambian su comportamiento basado en la entrada del usuario.\n",
    "- **Interactividad:** Pueden participar en diálogos y responder a preguntas o comandos.\n",
    "- **Extensibilidad:** Se les pueden añadir nuevas habilidades (herramientas) según las necesidades evolucionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Componentes Principales de un Agente.***\n",
    "\n",
    "Los agentes en LangChain se componen de varios elementos clave que permiten su funcionamiento:\n",
    "\n",
    "- **Modelo de Lenguaje (LLM):** El núcleo del agente, que procesa el lenguaje natural y genera respuestas.\n",
    "- **Herramientas (Tools):** Funciones o módulos que el agente puede utilizar para realizar tareas específicas, como buscar información en línea o acceder a bases de datos locales.\n",
    "- **AgentExecutor:** Un componente que ejecuta las acciones determinadas por el agente.\n",
    "\n",
    "### ***Diseñando e Implementando un Agente.***\n",
    "\n",
    "El diseño de un agente comienza con la definición clara del problema a resolver y la selección de las herramientas adecuadas para las tareas requeridas. Luego, se configura el modelo de lenguaje para que guíe al agente en la selección y ejecución de las acciones.\n",
    "\n",
    "***Pasos para implementar un agente:***\n",
    "\n",
    "1. Definir el problema y las tareas requeridas.\n",
    "2. Seleccionar y configurar las herramientas necesarias.\n",
    "3. Configurar el modelo de lenguaje para dirigir las acciones del agente.\n",
    "4. Implementar el AgentExecutor para ejecutar las acciones.\n",
    "5. Probar y ajustar el agente en entornos de prueba.\n",
    "\n",
    "### ***Gestionando y Mejorando Agentes***\n",
    "\n",
    "La gestión efectiva de un agente incluye el monitoreo continuo de su rendimiento, la actualización de sus herramientas y capacidades, y la adaptación a nuevos requisitos o datos. La retroalimentación de los usuarios y los datos de uso son cruciales para optimizar la funcionalidad y la precisión del agente.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/lacamposm/course-fundamentals-llms-openai-langchain/raw/main/images/agents_langchain.png\" alt=\"agent_image\" width=\"900\" height=\"450\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Componentes Principales de un Agente***\n",
    "\n",
    "\n",
    "LangChain facilita trabajar con agentes a través de varias abstracciones clave:\n",
    "\n",
    "- **AgentAction:** Clase que representa la acción que debe tomar el agente. Contiene propiedades para el nombre de la herramienta a invocar y la entrada para esa herramienta.\n",
    "- **AgentFinish:** Representa el resultado final de un agente, listo para ser devuelto al usuario. Contiene un mapeo de clave-valor con la salida final del agente.\n",
    "- **Intermediate Steps:** Representa acciones y salidas anteriores del agente en la ejecución actual. Es crucial para que el agente sepa qué ha hecho hasta ahora.\n",
    "\n",
    "\n",
    "### ***Conclusión***\n",
    "\n",
    "Los agentes en LangChain ofrecen una metodología poderosa y flexible para desarrollar aplicaciones de IA interactivas. Comprender cómo configurar, ejecutar y gestionar estos agentes es esencial para los desarrolladores que buscan aprovechar al máximo los modelos de lenguaje en aplicaciones reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (0.1.16)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.10/site-packages (1.13.3)\n",
      "Requirement already satisfied: langchain-openai in ./.venv/lib/python3.10/site-packages (0.1.3)\n",
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.10/site-packages (0.0.33)\n",
      "Requirement already satisfied: langchain-core in ./.venv/lib/python3.10/site-packages (0.1.43)\n",
      "Requirement already satisfied: langchain-text-splitters in ./.venv/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: chromadb in ./.venv/lib/python3.10/site-packages (0.4.24)\n",
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.10/site-packages (from langchain) (0.1.51)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.10/site-packages (from langchain) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.10/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.10/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in ./.venv/lib/python3.10/site-packages (from langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.10/site-packages (from chromadb) (1.1.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in ./.venv/lib/python3.10/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./.venv/lib/python3.10/site-packages (from chromadb) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.29.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.10/site-packages (from chromadb) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./.venv/lib/python3.10/site-packages (from chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.10/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.10/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.10/site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.10/site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.10/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.10/site-packages (from chromadb) (3.10.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in ./.venv/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in ./.venv/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.venv/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.2.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in ./.venv/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.21.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
      "Requirement already satisfied: streamlit in ./.venv/lib/python3.10/site-packages (1.33.0)\n",
      "Requirement already satisfied: wikipedia in ./.venv/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: numexpr in ./.venv/lib/python3.10/site-packages (2.10.0)\n",
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: langchainhub in ./.venv/lib/python3.10/site-packages (0.1.15)\n",
      "Requirement already satisfied: altair<6,>=4.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (5.3.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in ./.venv/lib/python3.10/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=16.8 in ./.venv/lib/python3.10/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (2.2.1)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in ./.venv/lib/python3.10/site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (15.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./.venv/lib/python3.10/site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./.venv/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in ./.venv/lib/python3.10/site-packages (from streamlit) (4.10.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./.venv/lib/python3.10/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in ./.venv/lib/python3.10/site-packages (from streamlit) (0.8.1b0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./.venv/lib/python3.10/site-packages (from streamlit) (6.4)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in ./.venv/lib/python3.10/site-packages (from streamlit) (4.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.10/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./.venv/lib/python3.10/site-packages (from langchainhub) (2.31.0.20240406)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./.venv/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.21.1)\n",
      "Requirement already satisfied: toolz in ./.venv/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Requirement already satisfied: langsmith in ./.venv/lib/python3.10/site-packages (0.1.51)\n",
      "Requirement already satisfied: duckduckgo-search in ./.venv/lib/python3.10/site-packages (5.3.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.10/site-packages (from langsmith) (3.10.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.10/site-packages (from langsmith) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langsmith) (2.31.0)\n",
      "Requirement already satisfied: click>=8.1.7 in ./.venv/lib/python3.10/site-packages (from duckduckgo-search) (8.1.7)\n",
      "Requirement already satisfied: curl-cffi>=0.6.2 in ./.venv/lib/python3.10/site-packages (from duckduckgo-search) (0.6.2)\n",
      "Requirement already satisfied: cffi>=1.12.0 in ./.venv/lib/python3.10/site-packages (from curl-cffi>=0.6.2->duckduckgo-search) (1.16.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from curl-cffi>=0.6.2->duckduckgo-search) (2024.2.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langsmith) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langsmith) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langsmith) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langsmith) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langsmith) (2.2.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.10/site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo-search) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas openpyxl langchain openai langchain-openai langchain-community langchain-core langchain-text-splitters chromadb pypdf\n",
    "!pip install streamlit wikipedia numexpr faiss-cpu langchainhub\n",
    "!pip install -U langsmith duckduckgo-search\n",
    "!pip install --upgrade --quiet langchain langchain-openai tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Ejemplo práctico:***\n",
    "\n",
    "Supongamos que queremos crear un agente que pueda responder preguntas actuales utilizando:\n",
    "\n",
    "- Una herramienta.\n",
    "- Dos herramientas.\n",
    "- Tres herramientas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una variable de entorno para la API KEY de OpenAI.\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR_LANGCHAIN_API_KEY>\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of September 2021, Leonardo DiCaprio is dating model Camila Morrone.', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 16, 'total_tokens': 35}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-977ece24-aefd-4a5b-a77b-73ccb76b7259-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm.invoke(\"Who is Leo DiCaprio's girlfriend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='El Mundial de Fútbol de 2022 se llevará a cabo en Qatar, por lo que aún no se ha disputado y no se conoce al país ganador.', response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 21, 'total_tokens': 60}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-98eafe78-8daf-45e0-a931-3d8193947934-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Que país ganó el mundial de futbol de 2022?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Tools***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n",
      "A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Page: 2022 FIFA World Cup\\nSummary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the world championship for national football teams organized by FIFA. It took place in Qatar from 20 November to 18 December 2022, after the country was awarded the hosting rights in 2010. It was the first World Cup to be held in the Arab world and Muslim world, and the second held entirely in Asia after the 2002 tournament in South Korea and Japan.\\nThis tournament was the last with 32 participating teams, with the number of teams being increased to 48 for the 2026 edition. To avoid the extremes of Qatar's hot climate, the event was held in November and December instead of during the traditional months of May, June, or July. It was held over a reduced time frame of 29 days with 64 matches played in eight venues across five cities. Qatar entered the event—their first World Cup—automatically as the host's national team, alongside 31 teams determined by the qualification process.\\nArgentina were crowned\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tool 1\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(doc_content_chars_max=1000)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "print(wiki_tool.name)\n",
    "print(wiki_tool.description)\n",
    "print(\"======================================\")\n",
    "wiki_tool.run({\"query\": \"World Cup 2022\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Agent 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1.\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent, create_tool_calling_agent\n",
    "from langchain.prompts import SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Eres un útil asistente hablando con un humano')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages[0] = SystemMessagePromptTemplate.from_template(\"Eres un útil asistente hablando con un humano\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `FIFA 2022 World Cup winner`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the world championship for national football teams organized by FIFA. It took place in Qatar from 20 November to 18 December 2022, after the country was awarded the hosting rights in 2010. It was the first World Cup to be held in the Arab world and Muslim world, and the second held entirely in Asia after the 2002 tournament in South Korea and Japan.\n",
      "This tournament was the last with 32 participating teams, with the number of teams being increased to 48 for the 2026 edition. To avoid the extremes of Qatar's hot climate, the event was held in November and December instead of during the traditional months of May, June, or July. It was held over a reduced time frame of 29 days with 64 matches played in eight venues across five cities. Qatar entered the event—their first World Cup—automatically as the host's national team, alongside 31 teams determined by the qualification process.\n",
      "Argentina were crowned\u001b[0m\u001b[32;1m\u001b[1;3mThe winner of the FIFA 2022 World Cup was Argentina.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Winner FIFA 2022 world cup.',\n",
       " 'output': 'The winner of the FIFA 2022 World Cup was Argentina.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [wiki_tool]\n",
    "agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Winner FIFA 2022 world cup.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Agent 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Actualmente, Leonardo DiCaprio no tiene una novia oficialmente confirmada. A lo largo de los años, ha tenido varias relaciones con diferentes mujeres, pero en la actualidad no se sabe con certeza si está en una relación sentimental.', response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 19, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-2bfcd7cb-bfd2-402c-97c1-cc1c11e522df-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Cuál es la novia actual de Leonardo Dicaprio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `novia actual de Leonardo Dicaprio`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mNo se encontró información sobre la novia actual de Leonardo DiCaprio en Wikipedia. ¿Hay algo más en lo que pueda ayudarte?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Cuál es la novia actual de Leonardo Dicaprio',\n",
       " 'output': 'No se encontró información sobre la novia actual de Leonardo DiCaprio en Wikipedia. ¿Hay algo más en lo que pueda ayudarte?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Cuál es la novia actual de Leonardo Dicaprio\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckduckgo_search\n",
      "A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "======================================\n",
      "Así es Vittoria Ceretti, la espectacular modelo de 25 años y ¿novia? de Leonardo DiCaprio, de 48 Según informan medios británicos, el actor y la maniquí italiana llevarían saliendo dos meses Parecía \"casualidad\", pero ha resultado ser verdad. Leonardo DiCaprio (48) tiene nueva novia y, como era de esperar, tiene 25 años.Las malas lenguas dicen que el actor de Titanic solo sale con chicas de menos de 25 años, y cuando superan esta cifra las deja, y su historial amoroso parece confirmarlo.. En noviembre de 2022 se relacionaba a El Renacido con la modelo y artista Victoria Lamas ... Las redes están que arden desde que, este martes, Leonardo DiCaprio y su novia, la modelo italiana Vittoria Ceretti, se dejaron ver en actitud relajada y cariñosa en un restaurante de Los Ángeles. A nadie se le escapó que la joven lucía un brillante anillo en el dedo anular de su mano izquierda, por lo que no tardaron en surgir rumores sobre un posible compromiso matrimonial de la pareja. La nueva novia de DiCaprio, quien ya ha pasado los 25 años, ha aparecido con un nuevo anillo. Y las redes ya sospechan sobre un compromiso con Vittoria Ceretti. Leonardo DiCaprio tendría una nueva relación amorosa con la modelo italiana Vittoria Ceretti, Según se pudo ver en unas fotografías difundidas por Daily Mail, que confirmarían que el actor ...\n"
     ]
    }
   ],
   "source": [
    "# Tool 2\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "api_wrapper_duck = DuckDuckGoSearchAPIWrapper()\n",
    "search_tool = DuckDuckGoSearchRun(api_wrapper=api_wrapper_duck)\n",
    "\n",
    "print(search_tool.name)\n",
    "print(search_tool.description)\n",
    "print(\"======================================\")\n",
    "print(search_tool.run({\"query\": \"Cuál es la novia actual de Leonardo Dicaprio\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `duckduckgo_search` with `{'query': 'Leonardo DiCaprio current girlfriend'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mA source said DiCaprio calls Ceretti his \"girlfriend.\" \"Vittoria is a beautiful girl, but Leonardo is of course surrounded by many beautiful girls most of the time,\" an insider told The ... Leonardo DiCaprio 'Seems to Really Like' Girlfriend Vittoria Ceretti and Their 'Date Nights' (Exclusive Source) Leonardo DiCaprio and Italian model Vittoria Ceretti have been romantically linked ... The actor, 49, and the model, 25, enjoyed a cozy meal at Cipriani in Beverly Hills on Friday night. They have been seen together several times since August 2023, when they kissed at a club in Ibiza. Published Dec. 18, 2023, 12:25 p.m. ET. Leonardo DiCaprio dined in Paris with Vittoria Ceretti on Thursday. SplashNews.com. Leonard DiCaprio is in the City of Love with Vittoria Ceretti. The actor ... Leonardo DiCaprio and his girlfriend, Italian model Vittoria Ceretti, have sparked engagement rumors. The speculation ignited as the 25-year-old Ceretti stepped out with DiCaprio, 49, sporting a ...\u001b[0m\u001b[32;1m\u001b[1;3mLa novia actual de Leonardo DiCaprio es la modelo italiana Vittoria Ceretti. Han sido vinculados románticamente desde agosto de 2023 y han sido vistos juntos varias veces desde entonces.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Quíen es la novia actual de Leonardo DiCaprio?',\n",
       " 'output': 'La novia actual de Leonardo DiCaprio es la modelo italiana Vittoria Ceretti. Han sido vinculados románticamente desde agosto de 2023 y han sido vistos juntos varias veces desde entonces.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [search_tool]\n",
    "agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Quíen es la novia actual de Leonardo DiCaprio?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Agent 3***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "2 elevado a la potencia 7 menos 5 divido 2\u001b[32;1m\u001b[1;3m```text\n",
      "2**7 - 5 / 2\n",
      "```\n",
      "...numexpr.evaluate(\"2**7 - 5 / 2\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m125.5\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer: 125.5\n"
     ]
    }
   ],
   "source": [
    "# Tool 3\n",
    "from langchain import LLMMathChain\n",
    "from langchain.tools import Tool\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "math_tool = Tool.from_function(\n",
    "    func=llm_math.run,\n",
    "    name=\"Calculator\",\n",
    "    description=\"Útil para responder preguntas de matemáticas. Sólo preguntas de matemáticas y nada más.\"\n",
    ")\n",
    "\n",
    "print(math_tool.invoke({\"question\": \"2 elevado a la potencia 7 menos 5 divido 2\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `Camila Morrone`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Camila Morrone\n",
      "Summary: Camila Rebeca Morrone (; born June 16, 1997) is an American actress and model. She made her acting debut in the James Franco film Bukowski (2013) and appeared in the action film Death Wish (2018), as well as the independent films Never Goin' Back (2018) and Mickey and the Bear (2019). Her role as Camila Alvarez-Dunne in the Amazon Prime Video limited series Daisy Jones & the Six (2023) earned her a nomination for a Primetime Emmy Award.\n",
      "\n",
      "Page: Marmalade (film)\n",
      "Summary: Marmalade is a 2024 romantic heist film written and directed by Keir O'Donnell in his directorial feature debut, and starring Joe Keery, Camila Morrone and Aldis Hodge.\n",
      "The film was released in cinemas and on demand on 9 February 2024.\n",
      "\n",
      "Page: Daisy Jones & the Six\n",
      "Summary: Daisy Jones & the Six is an American musical drama television miniseries developed by Scott Neustadter and Michael H. Weber, based on the 2019 novel of the same name by Taylor Jenkins Reid. Set in the Los Angeles music sce\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calculator` with `(2022-1997)^0.43`\n",
      "responded: La novia actual de Leonardo DiCaprio es Camila Morrone. Ella nació el 16 de junio de 1997. Ahora calcularé la edad actual de Camila Morrone elevada a la potencia 0.43.\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "(2022-1997)^0.43\u001b[32;1m\u001b[1;3m```text\n",
      "(2022-1997)**0.43\n",
      "```\n",
      "...numexpr.evaluate(\"(2022-1997)**0.43\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m3.991298452658078\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[38;5;200m\u001b[1;3mAnswer: 3.991298452658078\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calculator` with `2022-1997`\n",
      "responded: La novia actual de Leonardo DiCaprio es Camila Morrone. Ella nació el 16 de junio de 1997. Ahora calcularé la edad actual de Camila Morrone elevada a la potencia 0.43.\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "2022-1997\u001b[32;1m\u001b[1;3m```text\n",
      "2022-1997\n",
      "```\n",
      "...numexpr.evaluate(\"2022-1997\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m25\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[38;5;200m\u001b[1;3mAnswer: 25\u001b[0m\u001b[32;1m\u001b[1;3mLa edad actual de Camila Morrone es 25 años y al elevarla a la potencia 0.43, obtenemos aproximadamente 3.99.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '¿Quién es la novia actual de Leo DiCaprio?\\n¿Cuál es la edad actual de ella elevada a la potencia 0,43?',\n",
       " 'output': 'La edad actual de Camila Morrone es 25 años y al elevarla a la potencia 0.43, obtenemos aproximadamente 3.99.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [search_tool, wiki_tool, math_tool]\n",
    "agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"¿Quién es la novia actual de Leo DiCaprio?\\n¿Cuál es la edad actual de ella elevada a la potencia 0,43?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `Ollama`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Ulama (game)\n",
      "Summary: Ulama (Spanish pronunciation: [uˈlama]) is a ball game played in Mexico, currently experiencing a revival from its home in a few communities in the state of Sinaloa. As a descendant of the Aztec version of the Mesoamerican ballgame, the game is regarded as one of the oldest continuously played sports in the world and as the oldest known game using a rubber ball.\n",
      "\n",
      "Page: Mesoamerican ballgame\n",
      "Summary: Mesoamerican ballgame, Ollamaliztli, (Nahuatl languages: ōllamalīztli, Nahuatl pronunciation: [oːlːamaˈlistɬi], Mayan languages: pitz) was a sport with ritual associations played since at least 1650 BC by the pre-Columbian people of Ancient Mesoamerica.  The sport had different versions in different places during the millennia, and a modernized version of the game, ulama, is still played by the indigenous populations in some places.\n",
      "The rules of the Mesoamerican ballgame are not known, but judging from its descendant, ulama, they were probably similar to racquetb\u001b[0m\u001b[32;1m\u001b[1;3mEl Ollama es un juego de pelota jugado en México, que experimenta un renacimiento en algunas comunidades del estado de Sinaloa. Descendiente del juego de pelota mesoamericano de los aztecas, se considera uno de los deportes más antiguos del mundo que se juega de forma continua y el juego más antiguo conocido que utiliza una pelota de goma. El juego original, llamado Ollamaliztli en náhuatl, se practicaba desde al menos el 1650 a.C. por las civilizaciones precolombinas de Mesoamérica. Aunque las reglas exactas no se conocen, una versión modernizada del juego, llamada ulama, todavía se juega en algunas comunidades indígenas.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Dame un gran resumen de Ollama',\n",
       " 'output': 'El Ollama es un juego de pelota jugado en México, que experimenta un renacimiento en algunas comunidades del estado de Sinaloa. Descendiente del juego de pelota mesoamericano de los aztecas, se considera uno de los deportes más antiguos del mundo que se juega de forma continua y el juego más antiguo conocido que utiliza una pelota de goma. El juego original, llamado Ollamaliztli en náhuatl, se practicaba desde al menos el 1650 a.C. por las civilizaciones precolombinas de Mesoamérica. Aunque las reglas exactas no se conocen, una versión modernizada del juego, llamada ulama, todavía se juega en algunas comunidades indígenas.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Dame un gran resumen de Ollama\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Agent 4***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Eres un útil asistente hablando con un humano')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Ollama` with `{'query': 'summary'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mBlog · Ollama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Models\n",
      "Search\n",
      "Discord\n",
      "GitHub\n",
      "Download\n",
      "Sign in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Blog\n",
      "\n",
      "        Discord\n",
      "      \n",
      "\n",
      "        GitHub\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Models\n",
      "Sign in\n",
      "\n",
      "        Download\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              Llama 3 is not very censored\n",
      "            \n",
      "April 19, 2024\n",
      "Compared to Llama 2, Llama 3 feels much less censored. Meta has substantially lowered false refusal rates. Llama 3 will refuse less than 1/3 of the prompts previously refused by Llama 2.\n",
      "\n",
      "\n",
      "\n",
      "              Llama 3\n",
      "            \n",
      "April 18, 2024\n",
      "Llama 3 is now available to run on Ollama. This model is the next generation of Meta's state-of-the-art large language model, and is the most capable openly available LLM to date.\n",
      "\n",
      "\n",
      "\n",
      "              Embedding models\n",
      "            \n",
      "April 8, 2024\n",
      "Embedding models are available in Ollama, making it easy to generate vector embeddings for use in search and retrieval augmented generation (RAG) applications.\n",
      "\n",
      "Vision models\n",
      "            \n",
      "February 2, 2024\n",
      "New vision models are now available: LLaVA 1.6, in 7B, 13B and 34B parameter sizes. These models support higher resolution images, improved text recognition and logical reasoning.\n",
      "\n",
      "\n",
      "\n",
      "              Python & JavaScript Libraries\n",
      "            \n",
      "January 23, 2024\n",
      "The initial versions of the Ollama Python and JavaScript libraries are now available, making it easy to integrate your Python or JavaScript, or Typescript app with Ollama in a few lines of code. Both libraries include all the features of the Ollama REST API, are familiar in design, and compatible with new and previous versions of Ollama.\n",
      "\n",
      "\n",
      "\n",
      "              Building LLM-Powered Web Apps with Client-Side Technology\n",
      "            \n",
      "October 13, 2023\n",
      "Recreate one of the most popular LangChain use-cases with open source, locally running software - a chain that performs Retrieval-Augmented Generation, or RAG for short, and allows you to “chat with your documents”\n",
      "\n",
      "Ollama now supports AMD graphics cards\n",
      "            \n",
      "March 14, 2024\n",
      "Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.\n",
      "\n",
      "\n",
      "\n",
      "              Windows preview\n",
      "            \n",
      "February 15, 2024\n",
      "Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and serves the Ollama API including OpenAI compatibility.\n",
      "\n",
      "\n",
      "\n",
      "              OpenAI compatibility\n",
      "            \n",
      "February 8, 2024\n",
      "Ollama now has initial compatibility with the OpenAI Chat Completions API, making it possible to use existing tooling built for OpenAI with local models via Ollama.\n",
      "\n",
      "Run WizardMath model for math problems\n",
      "            \n",
      "August 14, 2023\n",
      "Run WizardMath model for math problems\n",
      "\n",
      "\n",
      "\n",
      "              Run Llama 2 uncensored locally\n",
      "            \n",
      "August 1, 2023\n",
      "This post will give some example comparisons running Llama 2 uncensored model versus its censored model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "© 2024 Ollama\n",
      "\n",
      "Blog\n",
      "Docs\n",
      "GitHub\n",
      "Discord\n",
      "X (Twitter)\n",
      "Meetups\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Blog\n",
      "Docs\n",
      "GitHub\n",
      "\n",
      "\n",
      "Discord\n",
      "X (Twitter)\n",
      "Meetups\n",
      "\n",
      "\n",
      "      © 2024 Ollama\u001b[0m\u001b[32;1m\u001b[1;3mOllama es una plataforma que ofrece modelos de lenguaje avanzados, como Llama 3, que se siente menos censurado en comparación con versiones anteriores. También ofrece modelos de visión como LLaVA 1.6 en diferentes tamaños de parámetros. Ollama proporciona bibliotecas en Python y JavaScript para facilitar la integración con sus modelos, y ahora es compatible con tarjetas gráficas AMD. Además, Ollama es compatible con Windows y tiene compatibilidad inicial con la API de Completado de Chat de OpenAI. También permite ejecutar modelos como WizardMath para resolver problemas matemáticos.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Dame un gran resumen de Ollama',\n",
       " 'output': 'Ollama es una plataforma que ofrece modelos de lenguaje avanzados, como Llama 3, que se siente menos censurado en comparación con versiones anteriores. También ofrece modelos de visión como LLaVA 1.6 en diferentes tamaños de parámetros. Ollama proporciona bibliotecas en Python y JavaScript para facilitar la integración con sus modelos, y ahora es compatible con tarjetas gráficas AMD. Además, Ollama es compatible con Windows y tiene compatibilidad inicial con la API de Completado de Chat de OpenAI. También permite ejecutar modelos como WizardMath para resolver problemas matemáticos.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tool 1\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Tool 2\n",
    "loader = WebBaseLoader(\"https://ollama.com/blog\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "vector = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"Ollama\",\n",
    "    description=\"Search for information about Ollama. For any questions about Ollama, you must use this tool!\",\n",
    ")\n",
    "\n",
    "\n",
    "tools = [search, retriever_tool]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Dame un gran resumen de Ollama\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `duckduckgo_search` with `{'query': 'clima en SF 26/04/2024'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mTemp. Dew Pt. This report shows the past weather for San Francisco, providing a weather history for 2024. It features all historical weather data series we have available, including the San Francisco temperature history for 2024. You can drill down from year to month and even day level reports by clicking on the graphs. Mar 22, 2024 Updated Mar 22, 2024. Meteorologists with the National Weather Service said that this weekend's storm system won't be a deluge by any means, but San Franciscans can still expect rain ... 26 Abr 2024, 06:20 AM EDT. ... El clima en Los Ángeles mañana habrá cielos despejados. Las temperaturas se moverán entre los 54 y los 72 grados Fahrenheit (12 y 22 grados Celsius). Las ... March 29. Most daily precipitation. 1.7 inches. January 31. Total annual precipitation. 18.0 inches. -. What's the hottest temperature in San Francisco so far this year? As of April 18, the highest temperature recorded in San Francisco, California in 2024 is 73 °F which has happened 3 times, most recently on April 17. ¡Que no te tome por sorpresa! Este será el clima para hoy 26 de abril de 2024 en México. El calor seguirá 'sofocando' al país este viernes en al menos 12 estados, con temperaturas de hasta 45 ...\u001b[0m\u001b[32;1m\u001b[1;3mNo pude encontrar información específica sobre el clima en San Francisco el 26 de abril de 2024. ¿Hay algo más en lo que pueda ayudarte?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '¿Cuál es el clima en SF 26/04/2024?',\n",
       " 'output': 'No pude encontrar información específica sobre el clima en San Francisco el 26 de abril de 2024. ¿Hay algo más en lo que pueda ayudarte?'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"¿Cuál es el clima en SF 26/04/2024?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Ollama` with `{'query': 'Cómo usar Ollama en mi computadora local'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mOllama is now available as an official Docker image\n",
      "            \n",
      "October 5, 2023\n",
      "Ollama can now run with Docker Desktop on the Mac, and run inside Docker containers with GPU acceleration on Linux.\n",
      "\n",
      "\n",
      "\n",
      "              Leveraging LLMs in your Obsidian Notes\n",
      "            \n",
      "September 21, 2023\n",
      "This post walks through how you could incorporate a local LLM using Ollama in Obsidian, or potentially any note taking tool.\n",
      "\n",
      "\n",
      "\n",
      "              How to prompt Code Llama\n",
      "            \n",
      "September 9, 2023\n",
      "This guide walks through the different ways to structure prompts for Code Llama and its different variations and features including instructions, code completion and fill-in-the-middle (FIM).\n",
      "\n",
      "\n",
      "\n",
      "              Run Code Llama locally\n",
      "            \n",
      "August 24, 2023\n",
      "Meta's Code Llama is now available on Ollama to try.\n",
      "\n",
      "\n",
      "\n",
      "              Run WizardMath model for math problems\n",
      "            \n",
      "August 14, 2023\n",
      "Run WizardMath model for math problems\n",
      "\n",
      "Ollama now supports AMD graphics cards\n",
      "            \n",
      "March 14, 2024\n",
      "Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.\n",
      "\n",
      "\n",
      "\n",
      "              Windows preview\n",
      "            \n",
      "February 15, 2024\n",
      "Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and serves the Ollama API including OpenAI compatibility.\n",
      "\n",
      "\n",
      "\n",
      "              OpenAI compatibility\n",
      "            \n",
      "February 8, 2024\n",
      "Ollama now has initial compatibility with the OpenAI Chat Completions API, making it possible to use existing tooling built for OpenAI with local models via Ollama.\n",
      "\n",
      "Vision models\n",
      "            \n",
      "February 2, 2024\n",
      "New vision models are now available: LLaVA 1.6, in 7B, 13B and 34B parameter sizes. These models support higher resolution images, improved text recognition and logical reasoning.\n",
      "\n",
      "\n",
      "\n",
      "              Python & JavaScript Libraries\n",
      "            \n",
      "January 23, 2024\n",
      "The initial versions of the Ollama Python and JavaScript libraries are now available, making it easy to integrate your Python or JavaScript, or Typescript app with Ollama in a few lines of code. Both libraries include all the features of the Ollama REST API, are familiar in design, and compatible with new and previous versions of Ollama.\n",
      "\n",
      "\n",
      "\n",
      "              Building LLM-Powered Web Apps with Client-Side Technology\n",
      "            \n",
      "October 13, 2023\n",
      "Recreate one of the most popular LangChain use-cases with open source, locally running software - a chain that performs Retrieval-Augmented Generation, or RAG for short, and allows you to “chat with your documents”\n",
      "\n",
      "Blog · Ollama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Models\n",
      "Search\n",
      "Discord\n",
      "GitHub\n",
      "Download\n",
      "Sign in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Blog\n",
      "\n",
      "        Discord\n",
      "      \n",
      "\n",
      "        GitHub\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Models\n",
      "Sign in\n",
      "\n",
      "        Download\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              Llama 3 is not very censored\n",
      "            \n",
      "April 19, 2024\n",
      "Compared to Llama 2, Llama 3 feels much less censored. Meta has substantially lowered false refusal rates. Llama 3 will refuse less than 1/3 of the prompts previously refused by Llama 2.\n",
      "\n",
      "\n",
      "\n",
      "              Llama 3\n",
      "            \n",
      "April 18, 2024\n",
      "Llama 3 is now available to run on Ollama. This model is the next generation of Meta's state-of-the-art large language model, and is the most capable openly available LLM to date.\n",
      "\n",
      "\n",
      "\n",
      "              Embedding models\n",
      "            \n",
      "April 8, 2024\n",
      "Embedding models are available in Ollama, making it easy to generate vector embeddings for use in search and retrieval augmented generation (RAG) applications.\u001b[0m\u001b[32;1m\u001b[1;3mPara usar Ollama en tu computadora local, ahora está disponible como una imagen oficial de Docker. Puedes ejecutar Ollama con Docker Desktop en Mac y dentro de contenedores Docker con aceleración de GPU en Linux. También puedes incorporar un LLM local utilizando Ollama en Obsidian u otras herramientas de toma de notas. Además, hay guías disponibles para ejecutar Code Llama localmente y utilizar el modelo WizardMath para problemas de matemáticas. Ollama ahora es compatible con tarjetas gráficas AMD en Windows y Linux, y también es compatible con la API de Completado de Chat de OpenAI. Puedes encontrar más información sobre cómo construir aplicaciones web con tecnología del lado del cliente impulsadas por LLM en el blog de Ollama.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"¿Cómo usar Ollama en mi computadora local?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para usar Ollama en tu computadora local, ahora está disponible como una imagen oficial de Docker. Puedes ejecutar Ollama con Docker Desktop en Mac y dentro de contenedores Docker con aceleración de GPU en Linux. También puedes incorporar un LLM local utilizando Ollama en Obsidian u otras herramientas de toma de notas. Además, hay guías disponibles para ejecutar Code Llama localmente y utilizar el modelo WizardMath para problemas de matemáticas. Ollama ahora es compatible con tarjetas gráficas AMD en Windows y Linux, y también es compatible con la API de Completado de Chat de OpenAI. Puedes encontrar más información sobre cómo construir aplicaciones web con tecnología del lado del cliente impulsadas por LLM en el blog de Ollama.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOGMBpGzRJu0U7Lmu1TXxkD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
