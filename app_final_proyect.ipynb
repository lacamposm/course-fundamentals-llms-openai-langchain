{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***ðŸ¦œ Chatbot - RAG OIC ðŸ¦œ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas openpyxl langchain openai langchain-openai langchain-community langchain-core langchain-text-splitters\n",
    "!pip install streamlit chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_final_proyect.py\n",
    "import os\n",
    "import shutil\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "save_rag_directory = \"RAG\"\n",
    "\n",
    "\n",
    "def save_uploaded_files(uploaded_files):\n",
    "    \"\"\"\n",
    "    Function that saves uploaded files in a specific directory and creates a document retriever.\n",
    "    \n",
    "    :param uploaded_files: list of files uploaded by the user.\n",
    "    :param save_rag_directory: directory where the files will be saved.\n",
    "    \n",
    "    Returns: None    \n",
    "    \"\"\"\n",
    "    if len(uploaded_files) > 0:\n",
    "        os.makedirs(save_rag_directory, exist_ok=True)\n",
    "        for uploaded_file in uploaded_files:\n",
    "            bytes_data = uploaded_file.read()\n",
    "            path_file = os.path.join(save_rag_directory, uploaded_file.name)\n",
    "            with open(path_file, \"wb\") as f:\n",
    "                f.write(bytes_data)\n",
    "                st.write(f\"Archivo {uploaded_file.name} guardado\")\n",
    "\n",
    "        retriever = create_retriever_documents()\n",
    "        st.session_state[\"retriever\"] = retriever\n",
    "        st.session_state[\"rag\"] = True\n",
    "        st.rerun()\n",
    "    else:\n",
    "        st.rerun()\n",
    "\n",
    "def chatbot_template(system_message, user_question):\n",
    "    \"\"\"\n",
    "    Create a template to pass custom queries to LLM \n",
    "    :param user_question: Message user.\n",
    "    :param system_message: Message to the system, like a rol.\n",
    "    :return: Configured with the system message, chat history, and user question.\n",
    "    \"\"\"\n",
    "    chat_prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            HumanMessagePromptTemplate.from_template(user_question)\n",
    "        ]\n",
    "    )\n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "def create_retriever_documents():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    loader = PyPDFDirectoryLoader(save_rag_directory)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=openai_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def stream_response_with_memory_openai(model_name, query, chat_history):\n",
    "    \"\"\"\n",
    "    Streams a response for a given query using an OpenAI model, formatted to maintain conversation history.\n",
    "\n",
    "    :param model_name: The name of the OpenAI model to use for generating responses.\n",
    "    :param query: The current user question or input.\n",
    "    :param chat_history: The history of previous interactions to be considered for context.\n",
    "    :return: A streaming object that continuously provides the chatbot's responses.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a friendly chatbot having a conversation with a human and giving answers only in \\\n",
    "    Spanish.\"\"\"\n",
    "    user_question = \"{user_question}\"\n",
    "    prompt = chatbot_template(system_message=system_prompt, user_question=user_question)\n",
    "    chat_openai = ChatOpenAI(model=model_name)\n",
    "\n",
    "    chain = (\n",
    "            {\"chat_history\": lambda x: chat_history, \"user_question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | chat_openai\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain.stream(query)\n",
    "\n",
    "\n",
    "def stream_rag_with_memory_openai(model_name, query, chat_history, retriever):\n",
    "    \"\"\"\n",
    "    Streams a response for a given query using an OpenAI model, formatted to maintain conversation history.\n",
    "\n",
    "    :param model_name: The name of the OpenAI model to use for generating responses.\n",
    "    :param query: The current user question or input.\n",
    "    :param chat_history: The history of previous interactions to be considered for context.\n",
    "    :return: A streaming object that continuously provides the chatbot's responses.\n",
    "    \"\"\"\n",
    "    chat_openai = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "    system_rag_prompt_template = \"\"\"You are a specialist in cadastre in the context of the country Colombia. \\\n",
    "    Answer the questions based only on the following context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    user_question = \"{user_question}\"\n",
    "    prompt = chatbot_template(system_message=system_rag_prompt_template, user_question=user_question)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"chat_history\": lambda x: chat_history, \"user_question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | chat_openai\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain.stream(query)\n",
    "\n",
    "\n",
    "# Interfaz de usuario #\n",
    "st.title(\"ðŸ¦œ Chatbot with RAG - OIC ðŸ¦œ\")\n",
    "\n",
    "if \"chat_active\" not in st.session_state:\n",
    "    with st.sidebar.expander(f\"Seleccione modelo y cargue sus archivos.\"):\n",
    "        model_openai = st.radio(\n",
    "            \"Seleccione su ChatModel:\",\n",
    "            [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\"],\n",
    "            key=\"openai_chat_model\"\n",
    "        )\n",
    "        st.session_state[\"openai_model\"] = model_openai\n",
    "        if os.path.exists(save_rag_directory):\n",
    "            delete_dir = st.button(f\"Borrar carpeta {save_rag_directory}\")\n",
    "            if delete_dir:\n",
    "                shutil.rmtree(save_rag_directory)\n",
    "                st.rerun()\n",
    "\n",
    "        uploaded_files = st.file_uploader(\"Elige los archivos PDF para el RAG\", accept_multiple_files=True,\n",
    "                                          type=[\"pdf\"])\n",
    "\n",
    "        if st.button(\"Finalizar carga de archivos y Chatear\"):\n",
    "            st.session_state[\"chat_active\"] = True\n",
    "            save_uploaded_files(uploaded_files)\n",
    "\n",
    "# =================================================================================== #\n",
    "# ====   Creamos secciÃ³n de chatbot con Memory en session_state de streamlit.  ====== #\n",
    "# =================================================================================== #\n",
    "if \"chat_active\" in st.session_state and \"rag\" in st.session_state:\n",
    "    st.text(f\"Archivos cargados para chatear:\\n {', '.join(os.listdir(save_rag_directory))}\")\n",
    "\n",
    "# Iniciamos un chat history y con streamlit creamos la memoria al estilo buffer #\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "for message in st.session_state.chat_history:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"Human\"):\n",
    "            st.markdown(message.content)\n",
    "    else:\n",
    "        with st.chat_message(\"AI\"):\n",
    "            st.markdown(message.content)\n",
    "\n",
    "user_query = st.chat_input(\"Your message\")\n",
    "\n",
    "if user_query is not None and user_query != \"\":\n",
    "    st.session_state.chat_history.append(HumanMessage(user_query))\n",
    "\n",
    "    with st.chat_message(\"Human\"):\n",
    "        st.markdown(user_query)\n",
    "\n",
    "    with st.chat_message(\"AI\"):\n",
    "        if os.path.exists(save_rag_directory) and \"retriever\" in st.session_state:\n",
    "            llm_response = st.write_stream(\n",
    "                stream_rag_with_memory_openai(\n",
    "                    model_name=st.session_state[\"openai_model\"],\n",
    "                    query=user_query,\n",
    "                    chat_history=st.session_state.chat_history,\n",
    "                    retriever=st.session_state[\"retriever\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            llm_response = st.write_stream(\n",
    "                stream_response_with_memory_openai(\n",
    "                    model_name=st.session_state[\"openai_model\"],\n",
    "                    query=user_query,\n",
    "                    chat_history=st.session_state.chat_history)\n",
    "            )\n",
    "\n",
    "    st.session_state.chat_history.append(AIMessage(llm_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run /content/app_final_proyect.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
